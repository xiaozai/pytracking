Training:  transformer  transformer50
loading annotations into memory...
Done (t=13.60s)
creating index...
index created!
/home/yans/pytracking-models/ltr/checkpoints/ltr/transformer/transformer50/DETR_ep*.pth
No matching checkpoint file found
song criterion: target and pred :  tensor([[193.6192,  17.5576,  53.5402,  63.1171],
        [108.7105, 126.3139,  47.4574,  37.5244],
        [155.6757, 105.3155,  34.1361,  78.4005],
        [ 99.0682, 126.5872,  43.4610,  50.7677],
        [165.6889,  84.1481,  32.0000,  91.9704],
        [236.0597, 112.8358,  36.8955,  94.5672],
        [109.3112, 114.3943,  57.4974,  83.6466],
        [262.2047,  67.4646,  15.8740,  18.9921],
        [188.2999, 168.3195,  34.1079,  69.4268],
        [ 21.2936,  84.6121,  78.5572, 103.6339],
        [107.6384,   0.0000, 189.6042,  50.4127],
        [139.6607, 120.0536,  62.5179,  78.1071],
        [179.0270, 136.5700,  60.1474,  25.8280],
        [ 36.6207, 138.4138,  39.1034,  45.3103],
        [130.4364, 109.5142,  45.1736,  72.1236],
        [175.7070,  60.5378,  49.9586,  33.8043],
        [ 31.9324,  46.1245,  39.2957, 141.8920],
        [147.7766, 101.9695,  47.0254,  66.5178],
        [ 77.2830, 134.8428,  22.5409,  46.2893],
        [205.1007, 113.8255,  52.1879,  34.3624],
        [166.9359, 131.6472,  57.6347, 142.5947],
        [152.2734,  52.3113,  75.2989,  98.6007],
        [242.9777,  82.1836,  33.2308,  34.3027],
        [212.9093, -16.0683,  80.2537, 137.0985],
        [233.7750, 139.5000,  67.0500, 141.7050],
        [-39.7751, 150.1881, 198.4476,  62.8930],
        [122.9866, 123.2444,  49.5040,  40.7377],
        [126.3300, 148.1298,  46.0367,  52.4006],
        [ 31.1741, 153.3158,  20.0114,  48.6593],
        [149.0688, 212.0833, 116.3520,  46.5408],
        [269.5963,  41.5911,  18.4038,  78.9329],
        [159.7855,  91.6802,  64.2451, 125.1814]], device='cuda:0') tensor([[142.0042, 153.8726, 145.4695, 145.5421],
        [151.2878, 143.9295, 152.4530, 143.7283],
        [144.1362, 146.3995, 151.1042, 151.8765],
        [141.0299, 147.8804, 147.5268, 149.6048],
        [142.8018, 152.8512, 142.3481, 145.7253],
        [138.0707, 150.4203, 147.1694, 140.6179],
        [149.7000, 146.8306, 150.7880, 145.9414],
        [141.1688, 147.1899, 150.0290, 138.0363],
        [143.3669, 147.2843, 144.1064, 142.2991],
        [136.9724, 150.2813, 142.1858, 149.1437],
        [144.1694, 148.1497, 152.6676, 143.6404],
        [139.2684, 151.9552, 145.0330, 145.6519],
        [139.8492, 151.2865, 145.1256, 144.3146],
        [137.1017, 153.8040, 145.4672, 147.7546],
        [148.2896, 148.1739, 148.5868, 145.1770],
        [145.1027, 144.3610, 144.8136, 145.0161],
        [134.3870, 149.0618, 144.7079, 141.3981],
        [141.3964, 145.0744, 148.5465, 146.6563],
        [147.5391, 148.6560, 145.1388, 148.6911],
        [143.6060, 154.2852, 150.1014, 153.3046],
        [142.8634, 151.8304, 145.8651, 148.8957],
        [136.3366, 155.5129, 148.2451, 139.2940],
        [139.0906, 147.6693, 152.0368, 141.3443],
        [135.1220, 149.8513, 146.8078, 143.3067],
        [145.6256, 152.2488, 145.3611, 144.5023],
        [144.7978, 148.6799, 142.7576, 143.9450],
        [141.6977, 146.7439, 147.9379, 145.6932],
        [143.8162, 148.4823, 145.0405, 144.7003],
        [133.1923, 152.6369, 149.8245, 148.1092],
        [144.0956, 150.0813, 146.0679, 147.3793],
        [142.4630, 152.9597, 153.4310, 141.1276],
        [140.2034, 143.7580, 142.8949, 145.0966]], device='cuda:0')
Training crashed at epoch 1
Traceback for the error!
Traceback (most recent call last):
  File "../ltr/trainers/base_trainer.py", line 70, in train
    self.train_epoch()
  File "../ltr/trainers/ltr_trainer.py", line 80, in train_epoch
    self.cycle_dataset(loader)
  File "../ltr/trainers/ltr_trainer.py", line 66, in cycle_dataset
    loss.backward()
  File "/home/yans/.conda/envs/pytracking/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/yans/.conda/envs/pytracking/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

Restarting training from last epoch ...
/home/yans/pytracking-models/ltr/checkpoints/ltr/transformer/transformer50/DETR_ep*.pth
No matching checkpoint file found
song criterion: target and pred :  tensor([[-10.8908,  99.2269,  62.9244,  81.0756],
        [130.3007, 101.2288,  49.5686,  90.3529],
        [214.1217,  31.3043,  85.1478, 165.2870],
        [ 42.1967, 105.0492,  98.2623,  69.0492],
        [ 68.2880,  93.0560,  84.5440,  34.1333],
        [112.6946, 186.7317,  48.5854,  59.3327],
        [210.7317, 125.5228,  27.4867,  26.5705],
        [143.1756, 107.1756,  40.1221,  92.6107],
        [177.1867,  90.1702,  19.2096,  68.0936],
        [170.6667, -54.8571,  64.0000, 141.7143],
        [126.8555, 160.3168,  50.6590,  51.0704],
        [169.3421, 184.5000,  20.1316,  52.1053],
        [ 66.6120, 119.7060,  89.2608,  21.8534],
        [ 61.8765,  93.8555,  60.9304,  73.0407],
        [191.1983, 184.9781,  19.1406,  53.9134],
        [172.8000,  88.1455,  66.3273,  44.5091],
        [ 22.3672,  75.5282,  43.5610, 117.5549],
        [129.2392, 115.4624,  82.6606,  49.8588],
        [ 58.8255, 114.7915,  44.5277, 100.9021],
        [188.6080,  56.5547,  67.8827,  31.6373],
        [ 48.1829, 100.3561,  41.2996,  63.4458],
        [111.3168, 223.9281,  58.8944,  65.3663],
        [121.8268, 110.4474,  23.4282,  66.1011],
        [  9.4359, 121.8462,  48.4103, 139.8974],
        [115.3954, 106.6167,  57.6095,  59.1440],
        [179.7941,  76.5257,  68.9829,  69.6686],
        [ 50.4612,   1.7236,  28.7778,  72.2780],
        [134.3139, 191.0665,  65.6706,  96.5694],
        [ 77.7810,  65.8686,  77.0803,  51.1533],
        [147.0901,  80.3434,  48.8240,  55.6223],
        [109.5780,  90.0526,  60.2595,  51.1701],
        [148.6625, 134.6750,  49.4944,  44.6526]], device='cuda:0') tensor([[139.1795, 147.9161, 149.9969, 141.2864],
        [147.0060, 147.2795, 147.0503, 147.3335],
        [142.8381, 148.4586, 143.1045, 144.0394],
        [139.8946, 148.8274, 146.4476, 149.9810],
        [141.6089, 148.3666, 146.6394, 146.4558],
        [141.0105, 149.6352, 144.1516, 147.9974],
        [133.2849, 148.1026, 150.5191, 142.6254],
        [146.4118, 146.3074, 153.9295, 146.6683],
        [135.4392, 150.5641, 144.9017, 147.1874],
        [145.4317, 152.3178, 141.6030, 147.6530],
        [137.9462, 145.6332, 146.1134, 147.0087],
        [143.0366, 154.2541, 149.4655, 148.5452],
        [141.7831, 150.6009, 154.3807, 152.3988],
        [138.7688, 145.5051, 151.6725, 142.5927],
        [143.5322, 149.3808, 148.5320, 143.6512],
        [136.8439, 150.8683, 149.9565, 148.5509],
        [136.6033, 146.0382, 150.7491, 142.8500],
        [143.4954, 142.7561, 150.1551, 143.0565],
        [142.2131, 149.6182, 144.4643, 137.9207],
        [149.7898, 152.2393, 146.4333, 143.5778],
        [136.6000, 142.8885, 146.6287, 141.2601],
        [140.2165, 145.4933, 151.7207, 142.4624],
        [140.9976, 146.2498, 150.0376, 141.4237],
        [136.2478, 149.1171, 141.8848, 149.7407],
        [141.9590, 144.7176, 148.9150, 144.8272],
        [139.2643, 154.0321, 140.8663, 143.9521],
        [143.3546, 155.8320, 151.9804, 145.8380],
        [145.6129, 154.5766, 145.3779, 140.0653],
        [137.1539, 146.0623, 146.4951, 150.1974],
        [146.6022, 145.8771, 148.4839, 143.3406],
        [148.6067, 148.9257, 152.8492, 149.4195],
        [140.5051, 151.2728, 149.2730, 148.4524]], device='cuda:0')
Training crashed at epoch 1
Traceback for the error!
Traceback (most recent call last):
  File "../ltr/trainers/base_trainer.py", line 70, in train
    self.train_epoch()
  File "../ltr/trainers/ltr_trainer.py", line 80, in train_epoch
    self.cycle_dataset(loader)
  File "../ltr/trainers/ltr_trainer.py", line 66, in cycle_dataset
    loss.backward()
  File "/home/yans/.conda/envs/pytracking/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/yans/.conda/envs/pytracking/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

Restarting training from last epoch ...
/home/yans/pytracking-models/ltr/checkpoints/ltr/transformer/transformer50/DETR_ep*.pth
No matching checkpoint file found
song criterion: target and pred :  tensor([[122.6118, 126.3605,  44.9277,  33.0600],
        [ 98.7026, 154.0465,  92.7099,  39.1285],
        [ 41.4721,  32.4720,  35.1360, 116.1360],
        [ 48.2297,  55.1196,  92.3254,  81.3014],
        [ -9.2571, 210.1812,  64.4180,  33.5314],
        [173.4940,  31.2289,  62.4578,  52.0482],
        [ 82.1563, 129.1243, 127.3120,  39.8700],
        [101.0880,  58.9440,  58.5120, 120.5280],
        [ 68.6378,  92.4011,  55.9390,  64.7263],
        [125.8142, 126.1610,  49.8500,  28.7901],
        [174.9359, 147.9662,  28.3449,  65.3564],
        [214.6545,  57.8502,  45.3572,  49.3492],
        [165.0347, 129.3973,  91.2107,  47.0933],
        [131.4287, 127.0450,  61.9009,  39.7827],
        [  5.9853, 220.4010,  55.2763,  55.2763],
        [171.1381, 124.9945,  65.7680,  28.4641],
        [176.5950,  34.0350,  41.2950,  33.9900],
        [166.7523, 137.9745,  77.8327,  66.4328],
        [168.5508,  91.4361,  26.4393,  80.7344],
        [120.9600, 189.5400,  70.3800, 103.8600],
        [ 92.4878, 151.8049,  37.4634,  51.1219],
        [139.2453,  98.4906,  38.7170, 118.1887],
        [171.6374, 111.8708, 174.3853,  19.7637],
        [ 39.9165, 127.7616,  52.5067,  62.0924],
        [ 65.9355, 135.6328,  26.9350,  81.2173],
        [103.5435, 123.6352,  80.9131,  43.6349],
        [110.6954, 118.0615,  33.6738, 142.1908],
        [142.7692, 124.9231,  18.8718,  18.6667],
        [ 75.0027,   9.4556,  35.7582,  39.3744],
        [110.9798, 140.0649, 105.6190,  44.7113],
        [116.5922, 130.6954,  43.8163,  23.2943],
        [176.6550, 107.9438,  50.8763,  24.0188]], device='cuda:0') tensor([[142.2944, 143.2211, 136.6179, 145.2309],
        [140.2660, 151.7501, 143.0501, 142.6221],
        [138.8127, 146.6600, 148.5409, 136.8417],
        [133.5719, 151.3550, 143.6240, 145.2050],
        [130.6681, 145.0882, 140.9525, 146.8004],
        [142.7666, 145.0212, 149.3457, 148.5797],
        [142.0956, 145.2544, 143.9741, 146.1153],
        [138.7620, 150.5118, 137.7635, 150.6889],
        [138.2057, 149.9078, 149.8400, 146.4026],
        [142.0773, 146.6611, 140.0289, 145.6412],
        [151.5704, 152.8874, 147.4169, 147.8642],
        [131.6496, 150.0375, 143.5925, 145.6842],
        [145.7555, 150.1595, 149.3498, 148.1154],
        [136.8865, 147.0073, 147.2188, 151.5233],
        [133.5906, 149.6690, 151.2877, 145.1857],
        [137.5170, 156.9713, 147.3019, 142.7882],
        [133.8482, 151.9732, 144.1987, 141.6776],
        [145.0163, 145.2932, 151.3083, 147.4892],
        [142.3026, 147.4596, 144.2606, 149.8027],
        [137.3428, 148.8848, 142.5677, 143.8673],
        [135.0716, 151.4038, 145.8782, 147.4062],
        [147.0551, 147.9077, 145.0352, 141.5446],
        [137.6404, 151.8334, 140.8186, 144.8708],
        [140.0119, 149.7247, 148.8042, 143.6127],
        [145.7565, 147.4650, 146.3844, 140.3984],
        [135.3472, 144.5330, 141.7175, 145.2982],
        [139.1570, 149.5955, 145.0908, 139.0909],
        [142.9469, 148.0502, 150.5946, 148.2060],
        [133.1676, 151.3594, 151.3483, 147.4319],
        [144.8037, 143.6936, 141.1650, 145.0556],
        [146.7696, 142.8638, 149.9660, 145.3635],
        [136.1610, 150.8799, 150.8523, 152.9851]], device='cuda:0')
Training crashed at epoch 1
Traceback for the error!
Traceback (most recent call last):
  File "../ltr/trainers/base_trainer.py", line 70, in train
    self.train_epoch()
  File "../ltr/trainers/ltr_trainer.py", line 80, in train_epoch
    self.cycle_dataset(loader)
  File "../ltr/trainers/ltr_trainer.py", line 66, in cycle_dataset
    loss.backward()
  File "/home/yans/.conda/envs/pytracking/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/yans/.conda/envs/pytracking/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

Restarting training from last epoch ...
/home/yans/pytracking-models/ltr/checkpoints/ltr/transformer/transformer50/DETR_ep*.pth
No matching checkpoint file found
